{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqa2a9ezo0ij"
      },
      "source": [
        "# Alle noodzakelijke informatie over je nieuwe API-aansluiting\n",
        "\n",
        "Hieronder vind je alle informatie die je nodig hebt om gebruik te kunnen maken van mistral medium 3 met je API Key. Belangerijkste punten op een rij:\n",
        "- Je API Key dat je eerder hebt gekregen blijft onverandert.\n",
        "- Je API verloopdatum blijft ook onverandert.\n",
        "- Vanaf nu tot 17/11/2025: je API geeft je toegang tot 2 modellen ipv 1; llama 3 70B en Mistral Medium 3.\n",
        "- Vanaf 17/11/2025: je API key geeft je alleen maar toegang tot Mistral Medium 3.\n",
        "- Je hoeft alleen maar 1 variable te wisselen om tussen models te wisselen en dat is je deployment naam.\n",
        "- In tegenstelling tot Llama is de chat template van Mistral restrictiever; alleen de rollen 'user', 'assistant', 'system' en 'tool' worden ondersteund. De chat template is [hier](https://github.com/vllm-project/vllm/blob/main/examples/tool_chat_template_mistral3.jinja) te raadplegen. Voor de meeste gebruikers en voor meeste use-cases is de 'user'-rol voldoende.\n",
        "\n",
        "Hieronder zal worden uitgelegd hoe je met mistral aan de salg kan gaan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "EoIxI1gfivIP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv(dotenv_path=\".env\")\n",
        "\n",
        "api_key = os.environ.get(\"VLAM_API_KEY\")\n",
        "base_url = os.environ.get(\"VLAM_BASE_URL\", \"https://api.demo.vlam.ai/v2.1/projects/poc/openai-compatible/v1\")\n",
        "model_id = os.environ.get(\"VLAM_MODEL_ID\", \"ubiops-deployment/bzk-bsw-chat//chat-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC4g-St_ZXaD"
      },
      "source": [
        "Wat heb je nodig om te starten:\n",
        "- De notebook die je van ons hebt ontvangen via email\n",
        "- Je API Key / UbiOps Token: ziet er zo uit \"Token 8736rvdf765ds98276edvfs7965dsf8762134\". Deze heb je via SecureFileTransfer of via Teams gekregen.\n",
        "- je deployment_name: bestaat uit drie delen en ziet er zo uit ==> min-afd-chat (ministerie - afdeling - chat).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Xgf-a44CpUXe"
      },
      "outputs": [],
      "source": [
        "# ubiops_token = \"Token a7da195f09c3743db7fb5c5f30c9e42cb894f7f0\" # verander dit mer je eigen Token\n",
        "ubiops_token = f\"Token {api_key}\" # verander dit mer je eigen Token\n",
        "\n",
        "##################################################################################################\n",
        "## plaats hieronder je deployment_name en vervang `chat` met `mistralmedium-flexibel`.\n",
        "## Voorbeelden:\n",
        "## bzk-rvb-chat    ==> bzk-rvb-mistralmedium-flexibel\n",
        "## logius-dig-chat ==> logius-kcc-mistralmedium-flexibel\n",
        "## vws-hos-chat    ==> vws-hos-mistralmedium-flexibel\n",
        "## ienw-pov-chat   ==> ienw-pov-mistralmedium-flexibel\n",
        "## ssc-test-chat ==> ssc-test-mistralmedium-flexibel\n",
        "\n",
        "# deployment_name = \"ssc-test-mistralmedium-flexibel\"\n",
        "deployment_name = \"bzk-bsw-chat\"\n",
        "# deployment_name = \"ssc-test-chat\"\n",
        "##################################################################################################\n",
        "\n",
        "\n",
        "## blijft onverandert\n",
        "project_name = \"poc\"\n",
        "version_name = \"\" # Leave blank to send request to the default version.\n",
        "model_name = \"chat-model\"\n",
        "base_url = f\"https://api.demo.vlam.ai/v2.1/projects/{project_name}/openai-compatible/v1\"\n",
        "model_id=f\"ubiops-deployment/{deployment_name}/{version_name}/{model_name}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdft-Y_Rfv4b"
      },
      "source": [
        "# **Vanaf hier is dit precies dezelfde als de notebook dat je van ons ooit hebt gekregen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBGPzmm6qe6u"
      },
      "source": [
        "De belangrijkste informatie die je nodig hebt is je **API Base URL** en je **Model-ID**. Dan kan je het in principe gebruiken op de manieren die OpenAI aanbied voor hun chatmodel. Bijvoorbeeld als je het model wilt intergreren met een user interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZFLZ0M0pgRW",
        "outputId": "360b5ad8-785c-49d3-efe5-58958ca99229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Base URL:\thttps://api.demo.vlam.ai/v2.1/projects/poc/openai-compatible/v1\n",
            "Model-ID: \tubiops-deployment/bzk-bsw-chat//chat-model\n"
          ]
        }
      ],
      "source": [
        "print(\"API Base URL:\\t\" + base_url)\n",
        "print(\"Model-ID: \\t\" + model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ikhNK2xpHmF"
      },
      "source": [
        "# Voorbeeld Python OpenAI interface\n",
        "\n",
        "De OpenAI API biedt een eenvoudige interface voor geavanceerde AI-modellen voor tekstgeneratie en natuurlijke taalverwerking. In dit voorbeeld wordt tekst gegenereerd op basis van een prompt, zoals je dat ook zou doen met ChatGPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8_7np7wmsqs",
        "outputId": "66c8a964-f184-4e3f-f962-16266b8a653b"
      },
      "outputs": [],
      "source": [
        "# %pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlkVV43xmx2f",
        "outputId": "8eba2724-aef3-4aac-9475-a0db83e9c504"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm all set and ready to go. How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Generate the OpenAI client\n",
        "client = OpenAI(\n",
        "    api_key=ubiops_token[6:] if ubiops_token.startswith(\"Token \") else ubiops_token,\n",
        "    base_url=base_url\n",
        ")\n",
        "# Send a (streaming) request to the deployment\n",
        "completion = client.chat.completions.create(\n",
        "    model=model_id,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Are you up and running?\"\n",
        "        }\n",
        "    ],\n",
        "    # temperature = 0.1,\n",
        "    stream=False\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJfNCMMxtz13"
      },
      "source": [
        "In de API-response kun je naast het gegenereerde antwoord van het model ook interessante metadata vinden, zoals:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x9xt-hGtzD9",
        "outputId": "00b3540a-1918-4fcd-d16e-50c0ac7ab30a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: llama-3.3-70b-instruct-awq\n",
            "ID: chatcmpl-270ec07422784ab7a876559c6c7e0652\n",
            "Prompt tokens: 41\n",
            "Completion tokens: 17\n",
            "Total tokens: 58\n"
          ]
        }
      ],
      "source": [
        "print(\"Model:\", completion.model)  # Welk model het antwoord genereerde\n",
        "print(\"ID:\", completion.id)        # Unieke ID van deze API-call (voor logging/debugging)\n",
        "print(\"Prompt tokens:\", completion.usage.prompt_tokens)        # Hoeveel tokens je input kostte\n",
        "print(\"Completion tokens:\", completion.usage.completion_tokens)  # Hoeveel tokens het antwoord kostte\n",
        "print(\"Total tokens:\", completion.usage.total_tokens)          # Totale kosten (input + output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2UMnI3vsZYm"
      },
      "source": [
        "# Voorbeeld met Curl\n",
        "\n",
        "Je kan het model ook gewoon rechtstreek benaderen via een HTTP verzoek met bijvoorbeeld Curl.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7l0Hwk3-NVIB"
      },
      "outputs": [],
      "source": [
        "################################ Hieronder moet je ook je Token invullen. En je deployment naam vervangen ###################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV1Y_6UUm6vv",
        "outputId": "e0f1b9c1-fe0c-4fd7-cd6b-7b3fc9af2f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"code\":\"404\",\"message\":\"Deployment was not found, please make sure it exists\",\"StatusCode\":404}"
          ]
        }
      ],
      "source": [
        "!curl -X POST \"https://api.demo.vlam.ai/v2.1/projects/poc/openai-compatible/v1/chat/completions\" \\\n",
        "  -H \"Authorization: Token ...\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"model\": \"ubiops-deployment/min-test-chat//chat-model\", \"messages\": [{\"role\": \"user\", \"content\": \"Are you up and running?\"}], \"stream\": false}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "!source .env\n",
        "# !echo \"$VLAM_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   769  100   627  100   142    335     76  0:00:01  0:00:01 --:--:--   411\n",
            "\u001b[1;39m{\n",
            "  \u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"chatcmpl-7d88012b92dc4e20a5602b5a8851323a\"\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"choices\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
            "    \u001b[1;39m{\n",
            "      \u001b[0m\u001b[1;34m\"finish_reason\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"stop\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[1;34m\"logprobs\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[1;34m\"message\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "        \u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"I'm all set and ready to go. How can I assist you today?\"\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"refusal\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"role\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"assistant\"\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"annotations\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"audio\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"function_call\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"tool_calls\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"reasoning_content\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m\n",
            "      \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[1;34m\"stop_reason\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m\n",
            "    \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m]\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"created\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1761124958\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"llama-3.3-70b-instruct-awq\"\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"object\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"chat.completion\"\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"service_tier\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"system_fingerprint\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"usage\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "    \u001b[0m\u001b[1;34m\"completion_tokens\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m17\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[1;34m\"prompt_tokens\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m41\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[1;34m\"total_tokens\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m58\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[1;34m\"completion_tokens_details\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[1;34m\"prompt_tokens_details\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"prompt_logprobs\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m\n",
            "\u001b[1;39m}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!curl -X POST \"https://api.demo.vlam.ai/v2.1/projects/poc/openai-compatible/v1/chat/completions\" \\\n",
        "  -H \"Authorization: Token $VLAM_API_KEY\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"model\": \"ubiops-deployment/bzk-bsw-chat//chat-model\", \"messages\": [{\"role\": \"user\", \"content\": \"Are you up and running?\"}], \"stream\": false}' | jq ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbhMugpfL7xz",
        "outputId": "e5b9d34a-9d5b-44af-f8ed-08d4a786c92e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"code\":\"404\",\"message\":\"Deployment was not found, please make sure it exists\",\"StatusCode\":404}"
          ]
        }
      ],
      "source": [
        "!curl -X POST \"https://api.demo.vlam.ai/v2.1/projects/poc/openai-compatible/v1/chat/completions\" \\\n",
        "  -H \"Authorization: Token ...\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"model\": \"ubiops-deployment/min-test-mistralmedium-flexibel//chat-model\", \"messages\": [{\"role\": \"user\", \"content\": \"Are you up and running?\"}], \"stream\": false}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1000  100   840  100   160    562    107  0:00:01  0:00:01 --:--:--   669\n",
            "\u001b[1;39m{\n",
            "  \u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"chatcmpl-a307454709494ae9bf4bb70f4f320194\"\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"choices\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[\n",
            "    \u001b[1;39m{\n",
            "      \u001b[0m\u001b[1;34m\"finish_reason\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"stop\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[1;34m\"index\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[1;34m\"logprobs\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[1;34m\"message\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "        \u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Yes! I'm fully operational and ready to help with anything you needâ€”whether it's answering questions, brainstorming ideas, solving problems, or just having a chat. ðŸ˜Š What can I do for you today?\"\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"refusal\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"role\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"assistant\"\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"annotations\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"audio\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"function_call\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"tool_calls\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m,\n",
            "        \u001b[0m\u001b[1;34m\"reasoning_content\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m\n",
            "      \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[1;34m\"stop_reason\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[1;34m\"token_ids\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m\n",
            "    \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m]\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"created\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1761125061\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"mistral-medium-2508\"\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"object\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"chat.completion\"\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"service_tier\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"system_fingerprint\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"usage\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "    \u001b[0m\u001b[1;34m\"completion_tokens\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m47\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[1;34m\"prompt_tokens\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m9\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[1;34m\"total_tokens\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m56\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[1;34m\"completion_tokens_details\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[1;34m\"prompt_tokens_details\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"prompt_logprobs\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"prompt_token_ids\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m,\n",
            "  \u001b[0m\u001b[1;34m\"kv_transfer_params\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;90mnull\u001b[0m\u001b[1;39m\n",
            "\u001b[1;39m}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!curl -X POST \"https://api.demo.vlam.ai/v2.1/projects/poc/openai-compatible/v1/chat/completions\" \\\n",
        "  -H \"Authorization: Token $VLAM_API_KEY\" \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"model\": \"ubiops-deployment/bzk-bsw-mistralmedium-flexibel//chat-model\", \"messages\": [{\"role\": \"user\", \"content\": \"Are you up and running?\"}], \"stream\": false}' | jq ."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
